{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, strip_accents_ascii\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from joblib import Parallel,delayed \n",
    "import multiprocessing as mp\n",
    "from collections import OrderedDict\n",
    "import faiss\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from time import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DATA_LOC': './../DATA', 'data_file_pattern': '**.json'}\n"
     ]
    }
   ],
   "source": [
    "REFRESH = True\n",
    "RAW_ID_KEY = 'id'\n",
    "json_data_key_1 = 'SpacyEnrichment'\n",
    "json_data_key_2 = 'tokens'\n",
    "json_data_key_3 = 'lemma'\n",
    "MIN_WORD_LEN = 3\n",
    "saved_data_loc = 'saved_model_data'\n",
    "pathobj = Path(saved_data_loc)\n",
    "pathobj.mkdir(parents=True, exist_ok=True)\n",
    "CONFIG = None\n",
    "with open(r'config.yaml') as fh:\n",
    "    CONFIG = yaml.load(fh, Loader=yaml.SafeLoader)\n",
    "print(CONFIG)\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(CONFIG['DATA_LOC'], CONFIG['data_file_pattern'])))\n",
    "\n",
    "file = files[0]\n",
    "file \n",
    "\n",
    "def aux_process_file(file):\n",
    "    global RAW_ID_KEY\n",
    "    file_data = {}\n",
    "    with open(file,'rb') as fh:\n",
    "        for line in fh:    \n",
    "            _dict = json.loads(line)\n",
    "            file_data[_dict[RAW_ID_KEY]] = _dict\n",
    "            \n",
    "    return aux_process_fileData(file_data)       \n",
    "\n",
    "\n",
    "# d1 = aux_process_file(file)\n",
    "# list(d1.keys())[0]\n",
    "\n",
    "def aux_process_fileData(file_data):\n",
    "    global MIN_WORD_LEN\n",
    "    global json_data_key_3\n",
    "    docID_words_dict = {}\n",
    "    for docID, doc in file_data.items():\n",
    "        words = []\n",
    "        for token in doc [json_data_key_1][json_data_key_2]:\n",
    "            if len(token[json_data_key_3]) > MIN_WORD_LEN:  words.append(token['lemma'])\n",
    "        docID_words_dict[docID] = words\n",
    "    return docID_words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_DICT_FILE = os.path.join(saved_data_loc,'documents_dict.pkl')\n",
    "\n",
    "if REFRESH or not os.path.exists(DOC_DICT_FILE):\n",
    "    list_docs = Parallel(mp.cpu_count())(delayed(aux_process_file)(file) for file in files)\n",
    "    # ----------------------\n",
    "    # Collate \n",
    "    # ----------------------\n",
    "    docID_words_dict = {}\n",
    "    for _ in list_docs:\n",
    "        docID_words_dict.update(_)\n",
    "\n",
    "    docID_words_dict = OrderedDict(docID_words_dict)\n",
    "    with open(DOC_DICT_FILE,'wb') as fh:\n",
    "        pickle.dump(\n",
    "            docID_words_dict,\n",
    "            fh,\n",
    "            pickle.HIGHEST_PROTOCOL\n",
    "        )\n",
    "else:\n",
    "    with open(DOC_DICT_FILE,'wb') as fh:\n",
    "        docID_words_dict = pickle.load(fh)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ----------------------------------------------------\n",
    "# Create synthetic IDs\n",
    "# -----------------------------------------------------\n",
    "docID_list = list(docID_words_dict.keys())\n",
    "docID_to_synID = {e[1]:e[0] for e in enumerate(docID_list,0)}\n",
    "docID_synID_df = pd.DataFrame( {'docID':docID_list, 'synID': np.arange(len(docID_list))}, columns=['docID','synID'])\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Create TF IDF vectorizer\n",
    "# ------------------------------------------------------\n",
    "t0 = time()\n",
    "kwvectorizer = TfidfVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    ngram_range=(1, 1), \n",
    "    stop_words='english', \n",
    "    min_df=0.01, \n",
    "    max_df=0.80\n",
    ")\n",
    "documents = list(docID_words_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Time taken for TF-IDF 1097.8843\n",
      " Time taken for SVD 308.7081\n"
     ]
    }
   ],
   "source": [
    "xformed_docs = kwvectorizer.fit_transform([ ' '.join(_) for  _ in documents])\n",
    "t1 = time()\n",
    "print(' Time taken for TF-IDF {:.4f}'.format(t1-t0))\n",
    "t0 = time()\n",
    "svd_obj = TruncatedSVD(n_components=128)\n",
    "xformed_docs_SVD = svd_obj.fit_transform(xformed_docs.todense())\n",
    "t1 = time()\n",
    "print(' Time taken for SVD {:.4f}'.format(t1-t0))\n",
    "xformed_docs_SVD = xformed_docs_SVD.astype(np.float32)\n",
    "xformed_docs_SVD.shape, type(xformed_docs_SVD)\n",
    "\n",
    "vectors = xformed_docs_SVD\n",
    "input_count = vectors.shape[0] #Input count\n",
    "d = vectors.shape[1] # Input dimension \n",
    "m = 16\n",
    "k = 4\n",
    "quantizer = faiss.IndexFlatL2(d)  # this remains the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Index Trained ? True\n"
     ]
    }
   ],
   "source": [
    "index = faiss.IndexIVFPQ(\n",
    "    quantizer, \n",
    "    d, \n",
    "    input_count, \n",
    "    m, \n",
    "    k\n",
    ")\n",
    "arr_ids = docID_synID_df['synID'].values.astype(np.int)\n",
    "\n",
    "t0 = time()\n",
    "index.train(vectors)\n",
    "t1 = time()\n",
    "print('Is Index Trained ?', index.is_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for Train 11.81667 ||  Adding index 1880.60822\n"
     ]
    }
   ],
   "source": [
    "index.add_with_ids(\n",
    "    vectors, \n",
    "    arr_ids \n",
    ")\n",
    "t2 = time()\n",
    "print('Time taken for Train {:.5f} ||  Adding index {:.5f}'.format(t1-t0, t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Save file\n",
    "# ------------------------------------------------------------------\n",
    "filename = os.path.join(saved_data_loc,'faiss_index.dat')\n",
    "faiss.write_index(index, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 2.3863\n",
      "Time taken 2.6693\n",
      "Time taken 2.1779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.177917242050171"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "def benchmark_time(index, vectors, docs_count = 1000, find_nn=10):\n",
    "    index.nprobe = 10\n",
    "    idx = np.arange(vectors.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:docs_count]\n",
    "    t0 = time()\n",
    "    D, I = index.search(vectors[idx], find_nn) # sanity check\n",
    "    t1 = time()\n",
    "    print('Time taken {:.4f}'.format(t1-t0))\n",
    "    return (t1-t0)\n",
    "\n",
    "\n",
    "benchmark_time(index, vectors)\n",
    "\n",
    "benchmark_time(index, vectors)\n",
    "\n",
    "benchmark_time(index, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

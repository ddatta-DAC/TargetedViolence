{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "INFO: Pandarallel will run on 40 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sBert\n",
    "\n",
    "import pandas as pd\n",
    "import sBert\n",
    "import LongFormer\n",
    "import \n",
    "import os\n",
    "import sys\n",
    "import faiss\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import joblib\n",
    "from joblib import Parallel,delayed\n",
    "from  tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import json\n",
    "from joblib import parallel_backend\n",
    "import datetime\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "SPACY_tokenizer = Tokenizer(nlp.vocab)\n",
    "import argparse\n",
    "import spacy\n",
    "import sys\n",
    "import faiss\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import glob\n",
    "import json\n",
    "from joblib import parallel_backend\n",
    "import datetime\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, strip_accents_ascii\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from joblib import Parallel,delayed \n",
    "import multiprocessing as mp\n",
    "from collections import OrderedDict\n",
    "import faiss\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "SPACY_tokenizer = Tokenizer(nlp.vocab)\n",
    "from nltk.corpus import stopwords\n",
    "nltk_stop_words = stopwords.words('english')\n",
    "from pandarallel import pandarallel\n",
    "from  tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "pandarallel.initialize()\n",
    "from collections import OrderedDict\n",
    "SPACY_NLP = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Editable location \n",
    "# =======================================================================================================================\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "DATA_DIR = os.path.join(home, 'hatespeech_prod_data','data', 'processed')\n",
    "model_pkl_dir = 'model_pkl_dir'\n",
    "# ===================================================================================================================================\n",
    "\n",
    "\n",
    "def custom_tokenizer_function (x):\n",
    "    global SPACY_tokenizer\n",
    "    global nltk_stop_words\n",
    "    doc = SPACY_tokenizer.__call__(x)\n",
    "    tokens = [str.lower(d.lemma_)  for d in doc ]\n",
    "    tokens = [ w for w in tokens if w not in nltk_stop_words and len(w)>1]\n",
    "    return tokens\n",
    "\n",
    "# ------------------------\n",
    "# Remove stopwords \n",
    "# ------------------------\n",
    "def clean_text_1(text):\n",
    "    global nltk_stop_words\n",
    "    remove_chars = ['\\n','\\t','!',',',';', ':', '?']\n",
    "    for c in remove_chars: text = text.replace(c,' ')\n",
    "    words = text.split(' ')\n",
    "    words = [w for w in words if not w in nltk_stop_words]\n",
    "    text = ' '.join(words)\n",
    "    text = ' '.join(custom_tokenizer_function (text))\n",
    "    return text\n",
    "\n",
    "# ------------------------\n",
    "# Not Remove stopwords \n",
    "# ------------------------\n",
    "def clean_text_2(text):\n",
    "    global SPACY_NLP\n",
    "    remove_chars = ['\\n','\\t','!',',',';', ':']\n",
    "    for c in remove_chars: \n",
    "        text = text.replace(c,' ')\n",
    "    sentences = [s.__repr__().strip() for s in SPACY_NLP(text).sents][:5]\n",
    "    text = ' '.join(sentences)\n",
    "    return text\n",
    "\n",
    "def get_doc_id(doc_dict):\n",
    "    return doc_dict.get('id')\n",
    "\n",
    "'''\n",
    "Save all resuts in a dataframe\n",
    "'''\n",
    "\n",
    "def process_doc(doc_dict):\n",
    "    res = {}\n",
    "    res['id'] = get_doc_id(doc_dict)\n",
    "    res['title'] = doc_dict['title']\n",
    "    res['xml_date'] = doc_dict['xml_date']\n",
    "    res['text'] = doc_dict['text']\n",
    "    res['complexId'] =  str(res['id']) + '_' + str(res['xml_date']) \n",
    "    return res\n",
    "\n",
    "def process_file(_file):\n",
    "    arr_docs = []\n",
    "    fh = open(_file,'r')\n",
    "    \n",
    "#     for l in fh.readlines():\n",
    "#         try:\n",
    "#             _doc = json.loads(l)\n",
    "#             arr_docs.append(_doc)\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "    def _aux_(l):\n",
    "        try:\n",
    "            _doc = json.loads(l)\n",
    "            return _doc\n",
    "        except:\n",
    "            pass \n",
    "    result = Parallel(n_jobs= mp.cpu_count())(delayed(_aux_)(l)  for l in fh.readlines() )    \n",
    "    fh.close() \n",
    "    \n",
    "    result = Parallel(n_jobs= mp.cpu_count())(delayed(process_doc)(_doc)  for _doc in result if _doc is not None) \n",
    "    filtered_results = []\n",
    "    for r in result:\n",
    "        r1 = r.copy()\n",
    "        r1['path']=_file\n",
    "        filtered_results.append(r1)\n",
    "    return filtered_results\n",
    "    \n",
    "def aux_process_subdir(sub_dir):\n",
    "    folder_files = glob.glob(os.path.join(sub_dir,'**.json'))\n",
    "    results = []\n",
    "    for file in folder_files:\n",
    "        res = process_file(file)\n",
    "        results.extend(res)\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==========\n",
    "# Format '2020-10-05'\n",
    "def get_date_range(_date_str, diff  ):\n",
    "    valid_dates = [(datetime.fromisoformat(_date_str) + timedelta(days=x)).strftime('%Y-%m-%d') for x in range(-diff, diff+1)]\n",
    "    return valid_dates\n",
    "\n",
    "def get_candidateDup_dateDirs(_dir_, diff = 5):\n",
    "    global DATA_DIR     \n",
    "    date_str = _dir_.split('/')[-1]\n",
    "    try:\n",
    "        dt_obj = datetime.fromisoformat(date_str)\n",
    "    except:\n",
    "        return []\n",
    "    valid_dates = get_date_range(date_str, diff = diff )\n",
    "    _parent = _dir_.replace(date_str,'')\n",
    "    valid_dirs = [ os.path.join(_parent, vd) for vd in valid_dates]\n",
    "    valid_dirs = [ _ for _ in valid_dirs if  os.path.exists(_)]\n",
    "    return valid_dirs\n",
    "\n",
    "# -------------------\n",
    "# Set up\n",
    "# 1. create emebedding(s) for each doc\n",
    "# 2. Primary key (id + xml_date)\n",
    "# 3. synthetic ID for each (+/-7 day period)\n",
    "# -------------------\n",
    "\n",
    "def process_all_files(dir_list):\n",
    "    # -----------------------------------\n",
    "    # Assign each to a separate thread\n",
    "    # -----------------------------------\n",
    "    results = []\n",
    "    collated = []\n",
    "    for i in tqdm(range(len(dir_list))):\n",
    "        r = aux_process_subdir(dir_list[i])\n",
    "        collated.extend(r)\n",
    "    \n",
    "    synID = 0 \n",
    "    for i in range(len(collated)):\n",
    "        collated[i]['synID'] = synID\n",
    "        synID += 1\n",
    "    return collated\n",
    "\n",
    "\n",
    "def obtain_value_list(list_data, key):\n",
    "    return [item[key] for item in list_data]\n",
    "\n",
    "def get_sBertEmbedding( list_dataDict ):\n",
    "    _text = obtain_value_list(list_dataDict, key='text') \n",
    "    _synID = obtain_value_list(list_dataDict, key='synID')\n",
    "    \n",
    "    # Preprocess the data for tokenizer\n",
    "    def aux_sb1( _id, _txt ):\n",
    "        return (_id, clean_text_2(_txt))\n",
    "    \n",
    "    def aux_sb2(_id, _txt ):\n",
    "        return (_id, sBert.get_doc_emb(_txt))\n",
    "    \n",
    "    results = []\n",
    "    with parallel_backend('threading', n_jobs=100):\n",
    "        results = Parallel()(delayed(aux_sb1)( _id,_txt)  for _id,_txt in zip(tqdm(_synID), _text) )\n",
    "    \n",
    "    results = Parallel(\n",
    "        n_jobs = 10\n",
    "    )(delayed(aux_sb2)( _id_txt[0],_id_txt[1]) for _id_txt in tqdm(results))\n",
    "    \n",
    "    results = OrderedDict({ item[0] : item[1] for item in results })\n",
    "    return results\n",
    "    \n",
    "\n",
    "def get_LongFormerEmbedding( list_dataDict ):\n",
    "    \n",
    "    _text = obtain_value_list(list_dataDict, key='text') \n",
    "    _synID = obtain_value_list(list_dataDict, key='synID')\n",
    "    \n",
    "    # Preprocess the data for tokenizer\n",
    "    def aux_sb1( _id, _txt ):\n",
    "        return (_id, clean_text_2(_txt))\n",
    "    \n",
    "    def aux_sb2(_id, _txt ):\n",
    "        return (_id, LongFormer.get_doc_emb(_txt))\n",
    "    \n",
    "    results = []\n",
    "    with parallel_backend('threading', n_jobs=100):\n",
    "        results = Parallel()(delayed(aux_sb1)( _id,_txt)  for _id,_txt in zip(tqdm(_synID), _text) )\n",
    "    \n",
    "    results = Parallel(\n",
    "        n_jobs = 10\n",
    "    )(delayed(aux_sb2)( _id_txt[0],_id_txt[1]) for _id_txt in tqdm(results))\n",
    "    \n",
    "    results = OrderedDict({ item[0] : item[1] for item in results })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_tfidfEmb(list_dataDict):\n",
    "    \n",
    "    kwvectorizer = TfidfVectorizer(\n",
    "        strip_accents='unicode',\n",
    "        ngram_range=(1, 1), \n",
    "        stop_words='english', \n",
    "        min_df=0.01, \n",
    "        max_df=0.90\n",
    "    )\n",
    "\n",
    "    _text = obtain_value_list(list_dataDict, key='text') \n",
    "    _synID = obtain_value_list(list_dataDict, key='synID')\n",
    "\n",
    "    def aux_1( _id, _txt ):\n",
    "        return (_id, clean_text_1(_txt))\n",
    "\n",
    "    results = []\n",
    "    with parallel_backend('threading', n_jobs=100):\n",
    "        results = Parallel()(delayed(aux_1)( _id,_txt)  for _id,_txt in zip(tqdm(_synID), _text) )\n",
    "\n",
    "\n",
    "    doc_list = [ _item[1]  for _item in results ]\n",
    "    id_list = [_item[0] for _item in results]\n",
    "\n",
    "    xformed_docs = kwvectorizer.fit_transform(doc_list)\n",
    "    svd_obj = TruncatedSVD(n_components=256)\n",
    "    svd_obj.fit(xformed_docs.todense())\n",
    "    vec = svd_obj.transform(xformed_docs.todense())\n",
    "    tfidfEmb = OrderedDict({\n",
    "        _id: _vec for _id,_vec in zip(id_list,vec)\n",
    "    })\n",
    "    return tfidfEmb\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "def set_up_index(\n",
    "    docId_list,\n",
    "    vectors, \n",
    "    _typeID,\n",
    "    refresh = False\n",
    "):\n",
    "    global model_pkl_dir\n",
    "    filename = os.path.join ( model_pkl_dir, 'faiss_index_{}'.format(_typeID))\n",
    "    if refresh is False and os.path.exists(filename):\n",
    "        index = faiss.read_index(filename)\n",
    "        return index\n",
    "    \n",
    "    input_count = vectors.shape[0]                  \n",
    "    d = vectors.shape[1]                            \n",
    "    m = 8\n",
    "    k = 128\n",
    "    print(d,m)\n",
    "    quantizer_text = faiss.IndexFlatL2(d) \n",
    "    \n",
    "    \n",
    "    index = faiss.IndexIVFFlat(quantizer_text, d, input_count)\n",
    "    \n",
    "    \n",
    "#     index = faiss.IndexIVFPQ(\n",
    "#         quantizer_text, \n",
    "#         d, \n",
    "#         input_count, \n",
    "#         m, \n",
    "#         k\n",
    "#     )\n",
    "\n",
    "    t0 = time()\n",
    "    index.train(vectors)\n",
    "    t1 = time()\n",
    "    print('Is Index Trained ?', index.is_trained)\n",
    "    t2 = time() \n",
    "    index.add_with_ids(\n",
    "        vectors, \n",
    "        np.array(docId_list).astype(np.int) \n",
    "    )\n",
    "    t3 = time()\n",
    "    print('Time taken for Train {:.5f} ||  Adding index {:.5f}'.format(t1-t0, t3-t2))\n",
    "    \n",
    "    # -----------------\n",
    "    # Save index \n",
    "    # -----------------\n",
    "    faiss.write_index(index, filename)\n",
    "   \n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "def setUp_index_byDate(date_str = None,  max_date_diff = 2):\n",
    "    global DATA_DIR\n",
    "    global model_pkl_dir\n",
    "    \n",
    "    pathobj = Path(model_pkl_dir)\n",
    "    pathobj.mkdir(exist_ok=True,parents=True)\n",
    "    if date_str is None:\n",
    "        return\n",
    "    date = date_str\n",
    "    list_sub_dirs = sorted([f.path for f in os.scandir(DATA_DIR) if f.is_dir()])\n",
    "    candidate_dirs = get_candidateDup_dateDirs( os.path.join(DATA_DIR, \"{}\".format(date)), diff = max_date_diff)\n",
    "   \n",
    "\n",
    "    #  res['text_no_SW'] = clean_text_1(doc_dict['text'])\n",
    "    #     res['text_w_SW'] = clean_text_2(doc_dict['text'])\n",
    "\n",
    "    preProc_dataFile =  os.path.join( model_pkl_dir, 'preProc_dataFile_{}.pkl'.format(date) )\n",
    "    print(preProc_dataFile)\n",
    "    if os.path.exists(preProc_dataFile):\n",
    "        with open(preProc_dataFile,'rb') as fh:\n",
    "            results = pickle.load(fh)\n",
    "    else:\n",
    "        # Obtain and save results\n",
    "        results = process_all_files(candidate_dirs)\n",
    "        with open(preProc_dataFile,'wb') as fh:\n",
    "            pickle.dump(results,fh, pickle.HIGHEST_PROTOCOL)\n",
    "    collated_data = results\n",
    "\n",
    "    # collated_data = collated_data[:500]\n",
    "\n",
    "    savefile_name_sBertEmbedding = os.path.join( model_pkl_dir, 'doc_id2sBertEmb_{}.pkl'.format(date))\n",
    "    savefile_name_LongFormerEmbedding = os.path.join( model_pkl_dir,'doc_id2LongFormerEmb_{}.pkl'.format(date))\n",
    "    savefile_name_tfidfEmbedding=  os.path.join( model_pkl_dir, 'doc_id2tfidfEmb_{}.pkl'.format(date))\n",
    "                                    \n",
    "    if os.path.exists(savefile_name_sBertEmbedding):\n",
    "        with open(savefile_name_sBertEmbedding,'rb') as fh:\n",
    "            sBertEmbedding = pickle.load(fh)\n",
    "    else:\n",
    "        sBertEmbedding = get_sBertEmbedding( collated_data )\n",
    "        with open(savefile_name_sBertEmbedding,'wb') as fh:\n",
    "            pickle.dump( sBertEmbedding, fh, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    if os.path.exists(savefile_name_LongFormerEmbedding):\n",
    "        with open(savefile_name_LongFormerEmbedding,'rb') as fh:\n",
    "            LongFormerEmbedding = pickle.load(fh)\n",
    "    else:\n",
    "        LongFormerEmbedding = get_LongFormerEmbedding(collated_data )\n",
    "        with open(savefile_name_LongFormerEmbedding,'wb') as fh:\n",
    "            pickle.dump( LongFormerEmbedding, fh, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "    \n",
    "    if os.path.exists(savefile_name_tfidfEmbedding):\n",
    "        with open(savefile_name_tfidfEmbedding,'rb') as fh:\n",
    "            doc_id2tfidfEmb = pickle.load(fh)\n",
    "    else:\n",
    "        doc_id2tfidfEmb =  get_tfidfEmb(collated_data)\n",
    "        with open(savefile_name_tfidfEmbedding,'wb') as fh:\n",
    "            pickle.dump( doc_id2tfidfEmb, fh, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \n",
    "    # Create mapping \n",
    "    synID = obtain_value_list(collated_data, 'synID') \n",
    "    _id = obtain_value_list(collated_data, 'id') \n",
    "    _path =  obtain_value_list(collated_data, 'path')\n",
    "    _title = obtain_value_list(collated_data, 'title')\n",
    "    xml_date = obtain_value_list(collated_data, 'xml_date') \n",
    "    \n",
    "    df_Mapping = pd.DataFrame({'synID':synID , 'id': _id, 'path': _path, 'title': _title, 'xml_date': xml_date})\n",
    "    df_Mapping.to_csv('mapping_data.csv',index=None)\n",
    "\n",
    "\n",
    "    # Create mapping\n",
    "    docId_list = obtain_value_list(collated_data,'synID')\n",
    "    vectors_text_tfidf = np.array(list(doc_id2tfidfEmb.values())).astype(np.float32)\n",
    "    vectors_text_sBert = np.array(list(sBertEmbedding.values())).astype(np.float32)\n",
    "    vectors_text_LFormer = np.array(list(LongFormerEmbedding.values())).astype(np.float32)\n",
    "\n",
    "    index_tfid = set_up_index(\n",
    "        docId_list = docId_list,\n",
    "        vectors= vectors_text_tfidf,\n",
    "        _typeID ='tfidf_{}'.format(date)\n",
    "    )\n",
    "\n",
    "    index_sBert = set_up_index(\n",
    "        docId_list = docId_list,\n",
    "        vectors= vectors_text_sBert,\n",
    "        _typeID ='sBert_{}'.format(date)\n",
    "    )\n",
    "\n",
    "    index_LFormer = set_up_index(\n",
    "        docId_list = docId_list,\n",
    "        vectors= vectors_text_LFormer,\n",
    "        _typeID ='LongFormer_{}'.format(date)\n",
    "    )\n",
    "    \n",
    "    return\n",
    "\n",
    "                                    \n",
    "# -----------------------------------------------\n",
    "# Function to set up all indices\n",
    "# -----------------------------------------------                                    \n",
    "def setup_all():\n",
    "    global DATA_DIR\n",
    "    list_sub_dirs = sorted([f.path for f in os.scandir(DATA_DIR) if f.is_dir()])\n",
    "    \n",
    "    valid_dates = []\n",
    "    for _subdir in list_sub_dirs:\n",
    "        date_str = _subdir.split('/')[-1]\n",
    "        try:\n",
    "            dt_obj = datetime.fromisoformat(date_str)\n",
    "        except:\n",
    "            continue\n",
    "        __date_str__= dt_obj.strftime('%Y-%m-%d')\n",
    "        valid_dates.append(__date_str__)\n",
    "    valid_dates = sorted(valid_dates)\n",
    "    \n",
    "    for _date_str in valid_dates:\n",
    "        setUp_index_byDate(_date_str)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_pkl_dir/preProc_dataFile_2021-01-06.pkl\n",
      "256 8\n",
      "Is Index Trained ? True\n",
      "Time taken for Train 0.00655 ||  Adding index 0.04977\n",
      "768 8\n",
      "Is Index Trained ? True\n",
      "Time taken for Train 0.03437 ||  Adding index 0.11157\n",
      "768 8\n",
      "Is Index Trained ? True\n",
      "Time taken for Train 0.03149 ||  Adding index 0.11499\n"
     ]
    }
   ],
   "source": [
    "setUp_index_byDate(date_str = '2021-01-06',  max_date_diff = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "setUp_index_byDate(date_str = '2021-01-01',  max_date_diff = 2)\n",
    "setUp_index_byDate(date_str = '2021-01-04',  max_date_diff = 2)\n",
    "setUp_index_byDate(date_str = '2021-01-05',  max_date_diff = 2)\n",
    "setUp_index_byDate(date_str = '2021-01-09',  max_date_diff = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import glob\n",
    "import os\n",
    "DATA_DIR = os.path.join('/home/ddatta', 'hatespeech_prod_data','data', 'processed')\n",
    "\n",
    "list_sub_dirs = sorted([f.path for f in os.scandir(DATA_DIR) if f.is_dir()])\n",
    "    \n",
    "valid_dates = []\n",
    "for _subdir in list_sub_dirs:\n",
    "    date_str = _subdir.split('/')[-1]\n",
    "    try:\n",
    "        dt_obj = datetime.fromisoformat(date_str)\n",
    "    except:\n",
    "        continue\n",
    "    __date_str__= dt_obj.strftime('%Y-%m-%d')\n",
    "    valid_dates.append(__date_str__)\n",
    "valid_dates = sorted(valid_dates)[-60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-12-06',\n",
       " '2020-12-07',\n",
       " '2020-12-08',\n",
       " '2020-12-09',\n",
       " '2020-12-10',\n",
       " '2020-12-11',\n",
       " '2020-12-12',\n",
       " '2020-12-13',\n",
       " '2020-12-14',\n",
       " '2020-12-15',\n",
       " '2020-12-16',\n",
       " '2020-12-17',\n",
       " '2020-12-18',\n",
       " '2020-12-19',\n",
       " '2020-12-20',\n",
       " '2020-12-21',\n",
       " '2020-12-22',\n",
       " '2020-12-23',\n",
       " '2020-12-24',\n",
       " '2020-12-25',\n",
       " '2020-12-26',\n",
       " '2020-12-27',\n",
       " '2020-12-28',\n",
       " '2020-12-29',\n",
       " '2020-12-30',\n",
       " '2020-12-31',\n",
       " '2021-01-01',\n",
       " '2021-01-02',\n",
       " '2021-01-03',\n",
       " '2021-01-04',\n",
       " '2021-01-05',\n",
       " '2021-01-06',\n",
       " '2021-01-07',\n",
       " '2021-01-08',\n",
       " '2021-01-09',\n",
       " '2021-01-10',\n",
       " '2021-01-11',\n",
       " '2021-01-12',\n",
       " '2021-01-13',\n",
       " '2021-01-14',\n",
       " '2021-01-15',\n",
       " '2021-01-16',\n",
       " '2021-01-17',\n",
       " '2021-01-18',\n",
       " '2021-01-19',\n",
       " '2021-01-20',\n",
       " '2021-01-21',\n",
       " '2021-01-22',\n",
       " '2021-01-23',\n",
       " '2021-01-24']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
